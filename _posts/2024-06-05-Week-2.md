---
layout: post
---

<!-- ## Difficulties

## Ideas

## Challenges

## Attempts to succeed

## Failures

## Advice -->

## Finished

### CWRU HPC Frequently Used Commands

- Start a job (switch from login-node `hpcxx` to computer-node `comptxx` or GPU-node `gputxx`)
    - To start a [batch job](https://sites.google.com/a/case.edu/hpcc/hpc-cluster/important-notes-for-new-users/scheduler-job-script?authuser=1) with a `.slurm` script
    ```bash
    sbatch xxx.slurm
    ```
    - To start an [interactive job](https://sites.google.com/a/case.edu/hpcc/hpc-cluster/important-notes-for-new-users/scheduler-job-script?authuser=1)
    ```bash
    srun --pty /bin/bash
    ```
- [Request GPU-nodes](https://sites.google.com/a/case.edu/hpcc/hpc-cluster/hardware/cluster-resources?authuser=1)
    - To check the [resource availability](https://ondemand-pioneer.case.edu/public/sinfo_pioneer.html)
    - To get information about GPU resources
    ```bash
    scontrol show node <gpu node>   # e.g. gput045
    ```
    - To request a GPU resource
    ```bash
    srun \
        -p gpu \        # change partition to gpu
        --gres=gpu:1 \  # request 1 GPU resource
        <other flags>
        --pty bash      # start an interactive job
    ```
- Working with directory on CWRU HPC
    > **_NOTE:_** We are supposed to store our large files in our gallina home. However, gallina home is not accessible in compute/gpu nodes.
    - Steps:
        - Create temporary scratch space
        - Copy data from RDS to /scratch
        - Run job script
        - Copy data from /scratch back to RDS
    - Refer to [Storing and using files on CWRU HPC and gallina](https://sites.google.com/case.edu/techne-public-site/storing-files-on-cwru-hpc-and-gallina?pli=1)
- Use `pip install` in HPC
    > **_NOTE:_** Most of the time HPC will have the packages installed, we should be careful to avoid over-installing packages. If any package is not found, contact professor/mentor first, and then the support team.

    - To [check and load available versions of Python](https://sites.google.com/a/case.edu/hpcc/hpc-cluster/pioneer-software/pioneer-alphabetical-list/k-q/python?authuser=1)

    ```bash
    # check available versions
    module avail python/
    module spider python/

    # load specific version
    module load Python/3.11.3
    ```

    - To [install modules with `pip`](https://sites.google.com/a/case.edu/hpcc/hpc-cluster/markov-software/software-installation-guide/installing-local-python-modules)

    ```bash
    pip install --user <package-name>
    ```

### LlamaIndex + RAG

- Studied [Building Agentic RAG with LlamaIndex - DeepLearning.AI course](https://www.deeplearning.ai/short-courses/building-agentic-rag-with-llamaindex/)
- Learned about LLM Agents, read [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/)

### Llama 2

- Acquired License for Meta Llama 2 and 3
- Attempted to run Llama 2 7B locally -- failed due to lack of GPU

## Other Messages From The Weekly Meeting

- Priority: Focus on the **generating** side first, put off the evaluation side.
- Finish something **small but working** first, add functionalities step by step.
- Possible roadmap regarding generating frame blending examples:
    - Run llama 2 with prompting
    - Run llama 2 with prompting and RAG on FrameNet dataset
    - Apply agents
    - Fine-tuning llama 2 with FrameNet dataset
    - ... (maybe)

## To-do List For Next Week

- Read paper [Lutma: A Frame-Making Tool for Collaborative FrameNet Development](https://aclanthology.org/2022.nlperspectives-1.13/)
- Contact Arthur about running Llama 2 on HPC, since he might have done it so that I won't have to do it all over again.
- Try to run Llama 2 and generate some simple frame blending examples through prompts.

## Weekly Meeting Slides Link

[Red Hen Meeting - Jun 5](https://docs.google.com/presentation/d/1U146xp9m_TG4WHnmPZS1GGsRoKtYivm2XTi7cTwUUuE/edit#slide=id.g2e30918e799_0_0)
